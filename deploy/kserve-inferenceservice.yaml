apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-onnx
  namespace: default
  annotations:
    serving.kserve.io/deploymentMode: "RawDeployment"
spec:
  predictor:
    serviceAccountName: sa
    model:
      modelFormat: 
        name: triton
      storageUri: "s3://models"
    # containers:
    #   - name: kserve-container
    #     image: nvcr.io/nvidia/tritonserver:xx.xx-py3-client
